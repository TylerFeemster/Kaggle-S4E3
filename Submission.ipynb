{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class paths:\n",
    "    TRAIN = './data/train.csv'\n",
    "    TEST = './data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engineering import full_feature_engineering\n",
    "\n",
    "train_df = pd.read_csv(paths.TRAIN)\n",
    "test_df = pd.read_csv(paths.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# From EnsembleOptimizing\n",
    "rf_weights = [0.44, 0.20, 0.14, 0.24, 0.36]\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "rf_cols = ['Y_Maximum', 'Y_Minimum', 'X_Maximum', 'X_Minimum',\n",
    "           'pca_37', 'pca_7']\n",
    "rf_df = full_feature_engineering(train_df, drop_cols=rf_cols)\n",
    "rf_test = full_feature_engineering(train_df, test_df=test_df, drop_cols=rf_cols)\n",
    "\n",
    "xgb_cols = ['Y_Maximum', 'Y_Minimum', 'X_Maximum', 'X_Minimum',\n",
    "            'Log_Outside_X_Index', 'Log_X_Perimeter', 'Log_Y_Perimeter',\n",
    "            'Log_Width', 'Log_Lum', 'Log_Height', 'Height', 'pca_3',\n",
    "            'pca_28', 'pca_25', 'pca_30', 'pca_33', 'pca_36', 'pca_31',\n",
    "            'pca_37', 'pca_35', 'pca_26', 'pca_23', 'pca_20', 'pca_19',\n",
    "            'pca_17', 'pca_16', 'pca_9', 'pca_2', 'pca_1', 'LogOfAreas',\n",
    "            'Edges_X_Index', 'pca_13', 'pca_29', 'pca_22', 'pca_14', 'pca_32',\n",
    "            'pca_34', 'pca_18', 'pca_27', 'pca_15', 'pca_11', 'Outside_Global_Index',\n",
    "            'pca_21', 'pca_10', 'pca_8', 'pca_7', 'Log_X_Index', 'pca_24', 'Log_Y_Index']\n",
    "xgb_df = full_feature_engineering(train_df, drop_cols=xgb_cols)\n",
    "xgb_test = full_feature_engineering(train_df, test_df=test_df, drop_cols=xgb_cols)\n",
    "\n",
    "y_cols = ['No Defect', 'Pastry', 'Z_Scratch', 'K_Scatch',\n",
    "          'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "real_cols = ['Pastry', 'Z_Scratch', 'K_Scatch',\n",
    "             'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "y = rf_df[y_cols]\n",
    "ids = train_df['id']\n",
    "\n",
    "X_rf = rf_df.drop(columns=[*y_cols, 'id'])\n",
    "X_xgb = xgb_df.drop(columns=[*y_cols, 'id'])\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 5000,\n",
    "    'max_samples': 0.20,\n",
    "    'max_features': 15,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 0,\n",
    "    'class_weight': 'balanced',\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': 1500,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 9,\n",
    "    'lambda': 1e-7,\n",
    "    'learning_rate': 0.0123,\n",
    "    'alpha': 3.059, \n",
    "    'subsample': 0.712\n",
    "}\n",
    "\n",
    "total_logits = None\n",
    "for fold, (train_index, valid_index) in enumerate(gkf.split(X_rf, y, ids)):\n",
    "    train_X_rf = X_rf.loc[train_index]\n",
    "    valid_X_rf = X_rf.loc[valid_index]\n",
    "\n",
    "    train_X_xgb = X_xgb.loc[train_index]\n",
    "    valid_X_xgb = X_xgb.loc[valid_index]\n",
    "\n",
    "    train_y = y.loc[train_index]\n",
    "    valid_y = y.loc[valid_index]\n",
    "\n",
    "    # RF\n",
    "    rf_model = RandomForestClassifier(**rf_params)\n",
    "    rf_model.fit(train_X_rf, train_y)\n",
    "\n",
    "    rf_test_preds = np.array(rf_model.predict_proba(rf_test.drop(columns='id')))[:, :, 1].T\n",
    "    rf_logits = np.log((0.00001 + rf_test_preds)/(1.00001 - rf_test_preds))\n",
    "\n",
    "    # XGB\n",
    "    xgb_model = XGBClassifier(**xgb_params)\n",
    "    xgb_model.fit(train_X_xgb, train_y, eval_set=[(valid_X_xgb, valid_y)], verbose=0)\n",
    "\n",
    "    xgb_test_preds = np.array(xgb_model.predict_proba(xgb_test.drop(columns='id')))\n",
    "    xgb_logits = np.log((0.00001 + xgb_test_preds)/(1.00001 - xgb_test_preds))\n",
    "\n",
    "    # Averaging logits according to weight\n",
    "    logits = rf_weights[fold] * rf_logits + (1 - rf_weights[fold]) * xgb_logits\n",
    "    if total_logits is not None:\n",
    "        total_logits += logits\n",
    "    else:\n",
    "        total_logits = logits\n",
    "\n",
    "exp_logits = np.exp(total_logits / 5)\n",
    "predictions = exp_logits / (1 + exp_logits)\n",
    "predictions /= predictions.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "submission_df = pd.DataFrame(test_df['id'], columns=['id'])\n",
    "submission_df[y_cols[1:]] = predictions[:,1:]\n",
    "submission_df.to_csv('./data/submission.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
